{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd8f5ee8-c8ce-47e4-909c-9d392cea6453",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "from idr_pytools import gpu_jobs_submitter, display_slurm_queue_jupyter, search_log, display_slurm_queue\n",
    "\n",
    "root = os.path.join(os.getenv(\"SCRATCH\"),\"consort-qa\")\n",
    "idris_model_dir = os.path.join(os.getenv(\"DSDIR\"),\"HuggingFace_Models\")\n",
    "scratch_model_dir = os.path.join(os.getenv(\"SCRATCH\"),\"models\")\n",
    "work_model_dir = os.path.join(os.getenv(\"WORK\"),\"models\",\"public\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cc22c6-839a-487e-b1d1-a027d0627553",
   "metadata": {},
   "source": [
    "# vLLM few-shot inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9d153ef-bc20-414c-ac62-575dec8a0aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = False\n",
    "\n",
    "base_cmd = \"python ./inference_vllm/{run_file}.py {prompt_dir} {model_path} {output_dir} \"\n",
    "\n",
    "# prompts dir\n",
    "prompts_dirs = glob.glob(f\"{root}/prompts/[0,1,3,5]-shot*\")\n",
    "\n",
    "models_paths_ngpus = [\n",
    "    (f'{idris_model_dir}/bigscience/bloomz-7b1', 1),\n",
    "    (f'{work_model_dir}/bloomz', 8),\n",
    "    (f'{work_model_dir}/gemma-7b-it', 1),\n",
    "    (f'{idris_model_dir}/microsoft/Phi-3-medium-4k-instruct', 1),\n",
    "    (f'{idris_model_dir}/meta-llama/Llama-2-7b-chat-hf', 1),\n",
    "    (f'{idris_model_dir}/meta-llama/Llama-2-70b-chat-hf', 2),\n",
    "    (f'{work_model_dir}/meditron-70b', 2),\n",
    "    (f'{idris_model_dir}/meta-llama/Meta-Llama-3-8B-Instruct', 1),\n",
    "    (f'{idris_model_dir}/meta-llama/Meta-Llama-3-70B-Instruct', 2),\n",
    "    (f'{work_model_dir}/Llama3-OpenBioLLM-8B', 1),\n",
    "    (f'{work_model_dir}/Llama3-OpenBioLLM-70B', 2),\n",
    "    (f'{work_model_dir}/c4ai-command-r-plus', 4),\n",
    "    (f'{work_model_dir}/Mistral-7B-Instruct-v0.3', 1),\n",
    "    (f'{idris_model_dir}/mistralai/Mixtral-8x22B-Instruct-v0.1', 8),\n",
    "    (f'{idris_model_dir}/mistralai/Mistral-7B-Instruct-v0.1', 1),\n",
    "    (f'{work_model_dir}/BioMistral-7B-DARE', 1),\n",
    "]\n",
    "no_5shot = [\"bloomz-7b1\",\"bloomz\",\"Llama-2-7b-chat-hf\",\"Llama-2-70b-chat-hf\",\"meditron-70b\",\"meditron-7b\",\"Phi-3-medium-4k-instruct\"]\n",
    "no_3shot = [\"bloomz-7b1\",\"bloomz\"]\n",
    "# commands\n",
    "exp_names = []\n",
    "exp_n_gpus = []\n",
    "cmds = []\n",
    "for prompt_dir in prompts_dirs :\n",
    "    prompt_strategy = prompt_dir.split('/')[-1]\n",
    "    for model_path,n_gpus in models_paths_ngpus :\n",
    "        model_name = model_path.split('/')[-1]\n",
    "        exp_name = f\"{prompt_strategy}_{model_name}\"\n",
    "        output_dir = f\"inference_vllm/out/{exp_name}\"\n",
    "        # run file \n",
    "        if \"-cot\" in prompt_strategy :\n",
    "            run_file = \"batched_cot\"\n",
    "        else :\n",
    "            run_file = \"batched_answer-gen\"\n",
    "        if \"5-shot\" in prompt_strategy and model_name in no_5shot:\n",
    "            continue\n",
    "        if \"3-shot\" in prompt_strategy and model_name in no_3shot:\n",
    "            continue\n",
    "        # check if current experience already done ( metrics available !)\n",
    "        if os.path.exists(f\"{output_dir}/metrics.json\"):continue\n",
    "        # fill command with necessary arguments\n",
    "        cmd = base_cmd.format(\n",
    "            run_file=run_file,\n",
    "            prompt_dir=prompt_dir,\n",
    "            model_path=model_path,\n",
    "            output_dir=output_dir,\n",
    "        )\n",
    "        cmd += f\"--n_gpus {n_gpus} \"\n",
    "        # use A100 for 70B models and V100 for smaller models\n",
    "        cmds.append(cmd)\n",
    "        exp_names.append(exp_name)\n",
    "        exp_n_gpus.append(n_gpus)\n",
    "        \n",
    "for i, exp in enumerate(exp_names):print(i,exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50e10c98-70c7-4d58-a545-c5a23a19f8e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['python ./inference_vllm/batched_cot.py /gpfsscratch/rech/aro/urz45id/consort-qa/prompts/3-shot-cot /gpfswork/rech/aro/urz45id/models/public/meditron-70b inference_vllm/out/3-shot-cot_meditron-70b --n_gpus 2 ', 'python ./inference_vllm/batched_cot.py /gpfsscratch/rech/aro/urz45id/consort-qa/prompts/3-shot-cot /gpfswork/rech/aro/urz45id/models/public/Llama3-OpenBioLLM-70B inference_vllm/out/3-shot-cot_Llama3-OpenBioLLM-70B --n_gpus 2 ']\n",
      "[2, 2]\n",
      "['3-shot-cot_meditron-70b', '3-shot-cot_Llama3-OpenBioLLM-70B']\n"
     ]
    }
   ],
   "source": [
    "if debug : # single exp for debug\n",
    "    chosen_index = 4 \n",
    "    exp_names = [exp_names[chosen_index]]\n",
    "    exp_n_gpus = [exp_n_gpus[chosen_index]]\n",
    "    cmds = [cmds[chosen_index]]\n",
    "print(cmds)\n",
    "print(exp_n_gpus)\n",
    "print(exp_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73b4466b-073a-4af5-a87f-98528ce03b42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch job 0: 2 GPUs distributed on 1 nodes with 2 tasks / 2 gpus per node and 8 cpus per task\n",
      "Submitted batch job 632364\n",
      "batch job 0: 2 GPUs distributed on 1 nodes with 2 tasks / 2 gpus per node and 8 cpus per task\n",
      "Submitted batch job 632378\n"
     ]
    }
   ],
   "source": [
    "slurm_addon_template = \"\"\"#SBATCH --mail-type=ALL\n",
    "#SBATCH --output=slurm/log/{exp_name}.out \n",
    "#SBATCH --error=slurm/log/{exp_name}.err\"\"\"\n",
    "\n",
    "script_addon = \"\"\"module load python/3.11.5\n",
    "conda activate vllm\n",
    "ray start --head\n",
    "\"\"\"\n",
    "\n",
    "for cmd,exp_name,n_gpus in zip(cmds, exp_names, exp_n_gpus) :\n",
    "    slurm_addon = slurm_addon_template.format(exp_name=exp_name)\n",
    "    gpu_jobs_submitter(\n",
    "        cmd,\n",
    "        name=exp_name,\n",
    "        n_gpu=n_gpus,\n",
    "        module=\"cuda/12.1.0\",\n",
    "        time_max=\"2:00:00\",\n",
    "        qos=None if not debug else \"qos_gpu-dev\",\n",
    "        account=\"aro@a100\",\n",
    "        slurm_addon=slurm_addon,\n",
    "        script_addon=script_addon \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40d611b3-b820-4de9-bf57-a5780c43ff14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n"
     ]
    }
   ],
   "source": [
    "!squeue -u $USER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73e87af5-5165-4c3d-8d3f-20cec44f99fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!scancel -u $USER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "efcc7608-5935-4f1a-86fd-f556872c3c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "!scancel 305249"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca5f9a4-a086-4873-a3b2-b7c92b893ff8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# sentence consort classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f9021e-d5c5-4bd8-ad3f-b2ab1bae073d",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug=True\n",
    "\n",
    "base_cmd = \"python ./consort-qa/predict_consort_sentence.py \"\n",
    "\n",
    "cmds = []\n",
    "\n",
    "cmd =  base_cmd\n",
    "cmd += f\"--model_path {scratch_model_dir}/biobert-consort-text \"\n",
    "cmd += f\"--data_path consort-qa/depression-crt/without_section.csv \"\n",
    "cmd += \"--text_column sentence_text \"\n",
    "#cmds.append(cmd)\n",
    "\n",
    "\n",
    "cmd =  base_cmd\n",
    "cmd += f\"--model_path {scratch_model_dir}/biobert-consort-text+section \"\n",
    "cmd += f\"--data_path consort-qa/depression-crt/with_section.csv \"\n",
    "cmd += \"--text_column model_input \"\n",
    "cmds.append(cmd)\n",
    "\n",
    "if debug: cmds=cmds[0]\n",
    "\n",
    "cmds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc5012c-991a-45ed-be20-9e0318275c75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# job name\n",
    "job_name = \"consort-sentence-classification\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330f994c-fa28-4e8d-a4ac-871aee8e4caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit jobs\n",
    "job_ids = gpu_jobs_submitter(\n",
    "    cmds,\n",
    "    name=job_name,\n",
    "    n_gpu=1,\n",
    "    qos=None if not debug else 'qos_gpu-dev',\n",
    "    module=\"pytorch-gpu/py3/2.1.1\",\n",
    "    time_max=\"04:00:00\" if not debug else \"00:05:00\",\n",
    "    account=\"aro@v100\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2247b135-9366-4c5b-b12a-b9bc05920457",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display_slurm_queue(job_name, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f37c36c-a0a0-4975-b5e3-257f0b5405ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logf_dict = search_log(job_name, with_err=True)\n",
    "ind=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09117e19-b55a-4b6d-9229-49e3d92fa93d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(open(logf_dict[\"stdout\"][ind]).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897d2fcf-0360-43e2-8308-24f12b48d6dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(open(logf_dict[\"stderr\"][ind]).read()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbf64f6-69d2-4386-9a11-dde2510e5d57",
   "metadata": {
    "tags": []
   },
   "source": [
    "# clean logs and debug outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6adc540c-8fed-47b0-95e3-3ea8516c5d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf slurm/*.slurm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60319b8b-6dc2-4482-b6f8-a1d7e47a5ac5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf slurm/log/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c4232b-1839-436f-8652-4cacbd978b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf inference/out/*_debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a2db37f-adbd-4757-90f0-4e6a558f59e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf core-*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77b9c51a-9980-4558-8710-5aff6d91bfde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# clean output dirs without predictions\n",
    "import shutil\n",
    "\n",
    "for out_dir in glob.glob(\"inference_vllm/out/*/\"):\n",
    "    if not os.path.exists(os.path.join(out_dir,\"predictions.json\")):\n",
    "        print(out_dir, \"removed\")\n",
    "        shutil.rmtree(out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1627af8-4013-44e7-aa6f-a97727de0459",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu-2.2.0_py3.11.7",
   "language": "python",
   "name": "module-conda-env-pytorch-gpu-2.2.0_py3.11.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
